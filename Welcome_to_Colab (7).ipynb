{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1  What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "   - K-Nearest Neighbors (KNN) is a simple, supervised learning algorithm that classifies new data points or predicts values based on the majority/average of their 'k' closest neighbors in the feature space, working on the principle that similar things exist near each other. For classification, it uses a majority vote (plurality) of labels from the k-nearest points to assign a class. For regression, it averages the continuous values of the k-nearest neighbors to predict a new value, often weighting closer neighbors more.\n",
        "\n",
        "\n",
        "### 2 What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "   - The Curse of Dimensionality means that as features (dimensions) increase, data becomes sparse, distances lose meaning, and algorithms struggle, especially KNN, which relies on distance; this causes poor generalization (overfitting), as neighbors become less distinct, making it hard to find truly \"close\" points and requiring exponentially more data to cover the space\n",
        "\n",
        "\n",
        "### 3  What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "   - Principal Component Analysis (PCA) is an unsupervised dimensionality reduction technique that transforms a large set of correlated variables into a smaller set of uncorrelated variables called Principal Components (PCs). By identifying directions (axes) where the data has the highest variance, PCA compresses data while preserving as much information—defined statistically as variance—as possible.The primary difference lies in how the number of features is reduced. Feature selection keeps a subset of the original variables, while PCA creates entirely new ones.\n",
        "Feature \tFeature Selection\tPCA (Feature Extraction)\n",
        "Output\tA subset of the original variables.\tEntirely new \"artificial\" variables (PCs).\n",
        "Data Integrity\tRetains original features and their physical meaning.\tCombines variables; new components are often hard to interpret.\n",
        "Approach\tDiscards \"unimportant\" columns entirely.\tProjects all original data into a lower-dimensional space.\n",
        "Information\tInformation in discarded features is lost.\tTries to preserve the \"signal\" from all features in the new components.\n",
        "Use Case\tBest when interpretability of specific variables is critical.\tBest for removing noise, handling multicollinearity, or visualizing complex data.\n",
        "\n",
        "### 4  What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "   - In PCA, eigenvectors define the new axes (Principal Components) showing directions of maximum data variance, while eigenvalues are scalar values indicating the amount of variance along those axes; they are crucial for dimensionality reduction, as they let us select components with the most significant information (highest eigenvalues) to simplify data while retaining most variability, making complex datasets easier to analyze.\n",
        "\n",
        "### 5  How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "   - In a single pipeline, Principal Component Analysis (PCA) acts as a crucial preprocessing step for the K-Nearest Neighbors (KNN) algorithm, addressing the \"curse of dimensionality\" and improving performance, efficiency, and accuracy.\n",
        "How PCA Complements KNN\n",
        "Dimensionality Reduction: KNN performance often suffers in high-dimensional spaces because distance calculations become less meaningful and computationally expensive (the \"curse of dimensionality\"). PCA transforms the data into a lower-dimensional space by identifying and retaining only the most informative features (principal components) that explain the most variance, effectively discarding redundant or noisy information.\n",
        "Improved Computational Efficiency: With fewer dimensions to process, the computational cost and memory requirements for KNN's distance calculations are significantly reduced. This leads to faster training and query times, which is especially beneficial for large datasets like image pixel data.\n",
        "Enhanced Accuracy: By removing noise and focusing on the most relevant features, PCA can help the KNN algorithm generalize better to unseen data, often leading to improved classification or regression accuracy compared to using the raw, high-dimensional data.\n",
        "Reduced Multicollinearity: PCA transforms correlated features into a set of new, uncorrelated components (orthogonal directions). This can be particularly helpful as the basic Euclidean distance metric used in KNN is sensitive to correlated features. The Pipeline in Practice\n",
        "In a typical machine learning pipeline, PCA is implemented as a transformation step before the KNN model is trained:\n",
        "Preprocessing/Standardization: The data is often standardized (mean-centered and scaled) before PCA is applied, as PCA is sensitive to the scale of the features.\n",
        "PCA Transformation: PCA is applied to the preprocessed data to reduce its dimensionality, with the number of principal components chosen to retain a sufficient amount of the original data's variance (e.g., 95%).\n",
        "KNN Model: The KNN algorithm is then applied to the new, lower-dimensional data to perform the final classification or regression task.\n",
        "\n",
        "### 6  Train a KNN Classifier on the Wine dataset with and without feature\n",
        "scaling. Compare model accuracy in both cases.\n",
        "  -"
      ],
      "metadata": {
        "id": "fotriVJd9MT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "feature_names = wine.feature_names\n",
        "target_names = wine.target_names\n",
        "\n",
        "# 2. Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# --- Scenario 1: Without Feature Scaling ---\n",
        "print(\"--- Without Feature Scaling ---\")\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5) # Using k=5 as an example\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(f\"Accuracy (Unscaled): {accuracy_unscaled:.4f}\")\n",
        "\n",
        "# --- Scenario 2: With Feature Scaling (StandardScaler) ---\n",
        "print(\"\\n--- With StandardScaler ---\")\n",
        "scaler_std = StandardScaler()\n",
        "X_train_scaled_std = scaler_std.fit_transform(X_train)\n",
        "X_test_scaled_std = scaler_std.transform(X_test)\n",
        "\n",
        "knn_scaled_std = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled_std.fit(X_train_scaled_std, y_train)\n",
        "y_pred_scaled_std = knn_scaled_std.predict(X_test_scaled_std)\n",
        "accuracy_scaled_std = accuracy_score(y_test, y_pred_scaled_std)\n",
        "print(f\"Accuracy (Standard Scaled): {accuracy_scaled_std:.4f}\")\n",
        "\n",
        "# --- Scenario 3: With Feature Scaling (MinMaxScaler) ---\n",
        "print(\"\\n--- With MinMaxScaler ---\")\n",
        "scaler_minmax = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train_scaled_minmax = scaler_minmax.fit_transform(X_train)\n",
        "X_test_scaled_minmax = scaler_minmax.transform(X_test)\n",
        "\n",
        "knn_scaled_minmax = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled_minmax.fit(X_train_scaled_minmax, y_train)\n",
        "y_pred_scaled_minmax = knn_scaled_minmax.predict(X_test_scaled_minmax)\n",
        "accuracy_scaled_minmax = accuracy_score(y_test, y_pred_scaled_minmax)\n",
        "print(f\"Accuracy (MinMax Scaled): {accuracy_scaled_minmax:.4f}\")\n",
        "\n",
        "# --- Comparison ---\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(f\"Unscaled Accuracy: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Standard Scaled Accuracy: {accuracy_scaled_std:.4f}\")\n",
        "print(f\"MinMax Scaled Accuracy: {accuracy_scaled_minmax:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUYyPdBC_glC",
        "outputId": "8d985e3b-bb96-48b0-a097-c68933d5bff1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Without Feature Scaling ---\n",
            "Accuracy (Unscaled): 0.7222\n",
            "\n",
            "--- With StandardScaler ---\n",
            "Accuracy (Standard Scaled): 0.9444\n",
            "\n",
            "--- With MinMaxScaler ---\n",
            "Accuracy (MinMax Scaled): 0.9630\n",
            "\n",
            "--- Comparison ---\n",
            "Unscaled Accuracy: 0.7222\n",
            "Standard Scaled Accuracy: 0.9444\n",
            "MinMax Scaled Accuracy: 0.9630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 7 : Train a PCA model on the Wine dataset and print the explained variance\n",
        "ratio of each principal component.\n"
      ],
      "metadata": {
        "id": "MPilxok-AHrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ade73f5",
        "outputId": "3c7a6ed3-027b-4b67-f834-95a5350acccc"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Standardize the data before applying PCA\n",
        "scaler_pca = StandardScaler()\n",
        "X_scaled_for_pca = scaler_pca.fit_transform(X)\n",
        "\n",
        "# Train a PCA model\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled_for_pca)\n",
        "\n",
        "# Print the explained variance ratio of each principal component\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "for i, ratio in enumerate(explained_variance_ratio):\n",
        "    print(f\"Principal Component {i+1}: {ratio:.4f}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "Principal Component 1: 0.3620\n",
            "Principal Component 2: 0.1921\n",
            "Principal Component 3: 0.1112\n",
            "Principal Component 4: 0.0707\n",
            "Principal Component 5: 0.0656\n",
            "Principal Component 6: 0.0494\n",
            "Principal Component 7: 0.0424\n",
            "Principal Component 8: 0.0268\n",
            "Principal Component 9: 0.0222\n",
            "Principal Component 10: 0.0193\n",
            "Principal Component 11: 0.0174\n",
            "Principal Component 12: 0.0130\n",
            "Principal Component 13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "432377f0"
      },
      "source": [
        "This output shows the proportion of variance in the original dataset that is captured by each principal component. You can decide how many components to keep based on the cumulative explained variance you want to achieve (e.g., 95% of the variance)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 8 Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset."
      ],
      "metadata": {
        "id": "j-EMkwGSA15K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e4a6dce"
      },
      "source": [
        "# Task\n",
        "To address the task, I'll first perform PCA to reduce the dimensionality of the Wine dataset to its top two principal components. Then, I'll train a K-Nearest Neighbors (KNN) classifier using this PCA-transformed data. Finally, I'll evaluate the KNN model's accuracy on the transformed test set and compare it with the accuracies obtained from the unscaled, Standard Scaled, and MinMax Scaled datasets.\n",
        "\n",
        "First, I'll apply PCA to the `X_train_scaled_std` and `X_test_scaled_std` data to retain the top 2 principal components.\n",
        "I'll begin by executing the code.\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Retain Top 2 Principal Components\n",
        "pca_2_components = PCA(n_components=2)\n",
        "X_train_pca = pca_2_components.fit_transform(X_train_scaled_std)\n",
        "X_test_pca = pca_2_components.transform(X_test_scaled_std)\n",
        "\n",
        "print(\"Explained Variance Ratio of the Top 2 Principal Components:\")\n",
        "for i, ratio in enumerate(pca_2_components.explained_variance_ratio_):\n",
        "    print(f\"Principal Component {i+1}: {ratio:.4f}\")\n",
        "print(f\"Cumulative Explained Variance: {pca_2_components.explained_variance_ratio_.sum():.4f}\")\n",
        "\n",
        "# Train KNN on PCA Data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "# Evaluate KNN on PCA Data\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "print(f\"\\nAccuracy (PCA-transformed with top 2 components): {accuracy_pca:.4f}\")\n",
        "\n",
        "# Compare Accuracies\n",
        "print(\"\\n--- Final Accuracy Comparison ---\")\n",
        "print(f\"Unscaled Accuracy: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Standard Scaled Accuracy: {accuracy_scaled_std:.4f}\")\n",
        "print(f\"MinMax Scaled Accuracy: {accuracy_scaled_minmax:.4f}\")\n",
        "print(f\"PCA-transformed (top 2 components) Accuracy: {accuracy_pca:.4f}\")\n",
        "\n",
        "# Summarize the impact of PCA\n",
        "print(\"\\n--- Impact of PCA on KNN Performance ---\")\n",
        "if accuracy_pca > accuracy_unscaled and accuracy_pca > accuracy_scaled_std and accuracy_pca > accuracy_scaled_minmax:\n",
        "    print(\"PCA with top 2 components achieved the highest accuracy among all methods tested.\")\n",
        "elif accuracy_pca > accuracy_unscaled:\n",
        "    print(\"PCA-transformed data significantly improved KNN accuracy compared to unscaled data.\")\n",
        "else:\n",
        "    print(\"PCA-transformed data improved KNN accuracy compared to unscaled data, but not as much as direct feature scaling methods.\")\n",
        "\n",
        "print(\"PCA helped reduce dimensionality while retaining a significant portion of the variance (approx. 55.41% with 2 components), which resulted in a competitive accuracy for the KNN classifier.\")\n",
        "print(\"While not surpassing the accuracies achieved by Standard Scaler or MinMax Scaler alone, PCA still provides a strong performance, especially considering the significant reduction in features (from 13 to 2). This demonstrates PCA's ability to extract meaningful information and mitigate the curse of dimensionality for KNN.\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9589341b"
      },
      "source": [
        "## Retain Top 2 Principal Components\n",
        "\n",
        "### Subtask:\n",
        "Apply PCA to the standardized training and testing datasets to reduce dimensionality to the top 2 principal components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12eb7d68"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires applying PCA to the standardized training and testing datasets to reduce dimensionality to the top 2 principal components. This involves creating a PCA instance, fitting it to the scaled training data, and then transforming both the scaled training and testing data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa924ccc",
        "outputId": "1552d7a3-d5cd-4190-9179-155f5bf19361"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a PCA instance with n_components=2\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Apply fit_transform to X_train_scaled_std to get X_train_pca\n",
        "X_train_pca = pca.fit_transform(X_train_scaled_std)\n",
        "\n",
        "# Apply transform to X_test_scaled_std to get X_test_pca\n",
        "X_test_pca = pca.transform(X_test_scaled_std)\n",
        "\n",
        "print(\"Shape of X_train_pca:\", X_train_pca.shape)\n",
        "print(\"Shape of X_test_pca:\", X_test_pca.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_pca: (124, 2)\n",
            "Shape of X_test_pca: (54, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 9 Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results.\n"
      ],
      "metadata": {
        "id": "T25TiM_cBcVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42ed589c"
      },
      "source": [
        "## Train KNN on PCA Data\n",
        "\n",
        "### Subtask:\n",
        "Train a K-Nearest Neighbors (KNN) classifier using the PCA-transformed training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90cd2019"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to train a K-Nearest Neighbors (KNN) classifier using the PCA-transformed training data. I will import the necessary class, instantiate the model, and then fit it to the PCA-transformed training data and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6072017d",
        "outputId": "3be9d4ab-320e-4632-97de-0c8f504c0263"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Instantiate a KNeighborsClassifier object with n_neighbors=5\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Fit the KNN model to the PCA-transformed training features and training labels\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "\n",
        "print(\"KNN model trained on PCA-transformed data.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN model trained on PCA-transformed data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 10"
      ],
      "metadata": {
        "id": "K8Mtxk0tBx_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98906453"
      },
      "source": [
        "## Evaluate KNN on PCA Data\n",
        "\n",
        "### Subtask:\n",
        "Predict the target values for the PCA-transformed test data using the trained KNN classifier and calculate the accuracy score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34a3e32e"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires making predictions on the PCA-transformed test data using the trained KNN classifier and then calculating the accuracy score. I will use the `predict` method of the `knn_pca` model and the `accuracy_score` function to achieve this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6cfc9b6",
        "outputId": "a2db1b13-f050-4f5d-884c-182f088f8fe2"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Use the knn_pca model to make predictions on the X_test_pca dataset\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "# 2. Calculate the accuracy of these predictions\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# 3. Print the calculated accuracy score\n",
        "print(f\"Accuracy (PCA-transformed with top 2 components): {accuracy_pca:.4f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (PCA-transformed with top 2 components): 0.9444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf12579c"
      },
      "source": [
        "## Compare Accuracy with Original Dataset\n",
        "\n",
        "### Subtask:\n",
        "Compare the accuracy of the KNN classifier on the PCA-transformed dataset with the accuracies obtained from the unscaled, Standard Scaled, and MinMax Scaled datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "190c03c1"
      },
      "source": [
        "**Reasoning**:\n",
        "I have already provided a markdown block to explain the comparison. Now I will generate the code block to perform the accuracy comparison as requested in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6145ee6b",
        "outputId": "84e1be66-d091-4d6b-c3ef-2cfe64b1f33a"
      },
      "source": [
        "print(\"\\n--- Final Accuracy Comparison ---\")\n",
        "print(f\"Unscaled Accuracy: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Standard Scaled Accuracy: {accuracy_scaled_std:.4f}\")\n",
        "print(f\"MinMax Scaled Accuracy: {accuracy_scaled_minmax:.4f}\")\n",
        "print(f\"PCA-transformed (top 2 components) Accuracy: {accuracy_pca:.4f}\")\n",
        "\n",
        "# Summarize the impact of PCA\n",
        "print(\"\\n--- Impact of PCA on KNN Performance ---\")\n",
        "if accuracy_pca > accuracy_unscaled and accuracy_pca > accuracy_scaled_std and accuracy_pca > accuracy_scaled_minmax:\n",
        "    print(\"PCA with top 2 components achieved the highest accuracy among all methods tested.\")\n",
        "elif accuracy_pca > accuracy_unscaled:\n",
        "    print(\"PCA-transformed data significantly improved KNN accuracy compared to unscaled data.\")\n",
        "else:\n",
        "    print(\"PCA-transformed data improved KNN accuracy compared to unscaled data, but not as much as direct feature scaling methods.\")\n",
        "\n",
        "print(\"PCA helped reduce dimensionality while retaining a significant portion of the variance (approx. 55.41% with 2 components), which resulted in a competitive accuracy for the KNN classifier.\")\n",
        "print(\"While not surpassing the accuracies achieved by Standard Scaler or MinMax Scaler alone, PCA still provides a strong performance, especially considering the significant reduction in features (from 13 to 2). This demonstrates PCA's ability to extract meaningful information and mitigate the curse of dimensionality for KNN.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Accuracy Comparison ---\n",
            "Unscaled Accuracy: 0.7222\n",
            "Standard Scaled Accuracy: 0.9444\n",
            "MinMax Scaled Accuracy: 0.9630\n",
            "PCA-transformed (top 2 components) Accuracy: 0.9444\n",
            "\n",
            "--- Impact of PCA on KNN Performance ---\n",
            "PCA-transformed data significantly improved KNN accuracy compared to unscaled data.\n",
            "PCA helped reduce dimensionality while retaining a significant portion of the variance (approx. 55.41% with 2 components), which resulted in a competitive accuracy for the KNN classifier.\n",
            "While not surpassing the accuracies achieved by Standard Scaler or MinMax Scaler alone, PCA still provides a strong performance, especially considering the significant reduction in features (from 13 to 2). This demonstrates PCA's ability to extract meaningful information and mitigate the curse of dimensionality for KNN.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3efe68e0"
      },
      "source": [
        "## Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "### Subtask:\n",
        "Train KNN classifiers with 'euclidean' and 'manhattan' distance metrics on the Standard Scaled Wine dataset and compare their accuracies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc4fa255"
      },
      "source": [
        "**Reasoning**:\n",
        "I will instantiate two `KNeighborsClassifier` models, one with 'euclidean' and another with 'manhattan' distance metrics, train them on the `X_train_scaled_std` data, make predictions on `X_test_scaled_std`, and then calculate and print their respective accuracy scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d19873b",
        "outputId": "8001e1ee-0e99-4332-958e-c6842f690f72"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Instantiate and train KNN with Euclidean metric\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled_std, y_train)\n",
        "\n",
        "# 2. Make predictions and calculate accuracy for Euclidean\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled_std)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "print(f\"Accuracy (Euclidean Distance): {accuracy_euclidean:.4f}\")\n",
        "\n",
        "# 3. Instantiate and train KNN with Manhattan metric\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled_std, y_train)\n",
        "\n",
        "# 4. Make predictions and calculate accuracy for Manhattan\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled_std)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "print(f\"Accuracy (Manhattan Distance): {accuracy_manhattan:.4f}\")\n",
        "\n",
        "# 5. Print both accuracy scores for comparison\n",
        "print(\"\\n--- Comparison of Distance Metrics ---\")\n",
        "print(f\"Euclidean Accuracy: {accuracy_euclidean:.4f}\")\n",
        "print(f\"Manhattan Accuracy: {accuracy_manhattan:.4f}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Euclidean Distance): 0.9444\n",
            "Accuracy (Manhattan Distance): 0.9815\n",
            "\n",
            "--- Comparison of Distance Metrics ---\n",
            "Euclidean Accuracy: 0.9444\n",
            "Manhattan Accuracy: 0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cac4ea3"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **How does the KNN model's accuracy on PCA-transformed data compare to other scaling methods?**\n",
        "    The KNN model achieved an accuracy of 0.9444 with PCA-transformed data (top 2 components). This is a significant improvement over the unscaled data's accuracy of 0.7222 and is comparable to the Standard Scaled data's accuracy of 0.9444. However, it was slightly lower than the MinMax Scaled data's highest accuracy of 0.9630.\n",
        "*   **Which distance metric (Euclidean or Manhattan) performed better for KNN on the scaled Wine dataset?**\n",
        "    The Manhattan distance metric achieved a higher accuracy of 0.9815, outperforming the Euclidean distance metric which yielded an accuracy of 0.9444 on the Standard Scaled Wine dataset.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   Principal Component Analysis (PCA) successfully reduced the dimensionality of the Wine dataset from 13 features to 2 principal components for both training and testing sets.\n",
        "*   The top 2 principal components collectively explained approximately 55.41% of the total variance in the dataset.\n",
        "*   A K-Nearest Neighbors (KNN) classifier trained on this 2-component PCA-transformed data achieved an accuracy of 0.9444.\n",
        "*   Comparing across different data preparations, the accuracies were:\n",
        "    *   Unscaled: 0.7222\n",
        "    *   Standard Scaled: 0.9444\n",
        "    *   MinMax Scaled: 0.9630\n",
        "    *   PCA-transformed (top 2 components): 0.9444\n",
        "*   When evaluating KNN on Standard Scaled data with different distance metrics, the Manhattan distance achieved an accuracy of 0.9815, which was higher than the Euclidean distance's accuracy of 0.9444.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   PCA effectively reduced dimensionality by 85% (from 13 to 2 features) while maintaining a high classification accuracy comparable to Standard Scaling. This suggests that the top two principal components capture the most critical information for classification in this dataset, mitigating the curse of dimensionality.\n",
        "*   The choice of distance metric significantly impacts KNN performance. In this case, the Manhattan distance yielded a superior accuracy of 0.9815 compared to Euclidean distance on the Standard Scaled data, indicating that it might be more suitable for this dataset's feature distribution. Further investigation into other distance metrics or hyperparameter tuning for KNN with Manhattan distance could lead to even better results.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}